#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Mar 10 08:21:50 2023

@author: lupus
"""

from time import sleep
import copy
import gymnasium as gym
from gymnasium.spaces import Discrete, Box, Tuple
import numpy as np


class MyRandomEnv(gym.Env):
    """A randomly acting environment.
    Can be instantiated with arbitrary action-, observation-, and reward
    spaces. Observations and rewards are generated by simply sampling from the
    observation/reward spaces. The probability of a `terminated=True` after each
    action can be configured, as well as the max episode length.
    """
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 4}

    def __init__(self, config=None, **kwargs): # kwargs added
        config = config or {}

        # Action space.
        self.action_space = config.get("action_space", Discrete(5))
        # Observation space from which to sample.
        self.observation_space = config.get(
            "observation_space",
            Box(low=0,high=255,shape=(72, 128, 3),dtype=np.uint8))
        # Reward space from which to sample.
        self.reward_space = config.get(
            "reward_space",
            Box(low=-1.0, high=1.0, shape=(), dtype=np.float32))
        self.static_samples = config.get("static_samples", False)
        if self.static_samples:
            self.observation_sample = self.observation_space.sample()
            self.reward_sample = self.reward_space.sample()

        # Chance that an episode ends at any step.
        # Note that a max episode length can be specified via
        # `max_episode_len`.
        self.p_terminated = config.get("p_terminated")
        if self.p_terminated is None:
            self.p_terminated = config.get("p_done", 0.1)
        # A max episode length. Even if the `p_terminated` sampling does not lead
        # to a terminus, the episode will end after at most this many
        # timesteps.
        # Set to 0 or None for using no limit on the episode length.
        self.max_episode_len = config.get("max_episode_len", None)
        # Whether to check action bounds.
        self.check_action_bounds = config.get("check_action_bounds", False)
        self.sleeping = config.get("sleeping", 0.0)
        # Steps taken so far (after last reset).
        self.steps = 0
        self.previous_seeding = None  # custom for use below
        
        # for rllib - custom seeding (not really needed)
        if config:
            
            try:
                _seed = int(config.worker_index * config.num_workers * (config.vector_index + 1))
                _seed = _seed if _seed >= 0 else abs(_seed)
                # We need the following line to seed self.np_random
                super().reset(seed=_seed)
                self.previous_seeding = _seed
                print("###################################") 
                print("Seeding succeded using env_config") 
                print("###################################") 

            except AttributeError:
                print("############################################################################################")
                print("Something went wrong when evaluating config in terms of worker, vector index or num_workers")
                print("############################################################################################")
                
        else:
            super().reset(seed=None)
            

    def reset(self, *, seed=None, options=None):
        
        print("I'm now being reset by a worker") # for debugging of Impala/RLLIB
        
        sleep(self.sleeping*3) # to mimic slow env
        
        if seed and seed != self.previous_seeding:
            # We need the following line to seed self.np_random
            super().reset(seed=seed)
            self.previous_seeding = seed
        
        self.steps = 0
        if not self.static_samples:
            return self.observation_space.sample(), {}
        else:
            return copy.deepcopy(self.observation_sample), {}

    def step(self, action):
        
        sleep(self.sleeping) # to mimic slow env
        
        if self.check_action_bounds and not self.action_space.contains(action):
            raise ValueError(
                "Illegal action for {}: {}".format(self.action_space, action)
            )
        if isinstance(self.action_space, Tuple) and len(action) != len(
            self.action_space.spaces
        ):
            raise ValueError(
                "Illegal action for {}: {}".format(self.action_space, action)
            )

        self.steps += 1
        terminated = False
        truncated = False
        # We are `truncated` as per our max-episode-len.
        if self.max_episode_len and self.steps >= self.max_episode_len:
            truncated = True
        # Max episode length not reached yet -> Sample `terminated` via `p_terminated`.
        elif self.p_terminated > 0.0:
            terminated = bool(
                self.np_random.choice(
                    [True, False], p=[self.p_terminated, 1.0 - self.p_terminated]
                )
            )
            
        if not self.static_samples:
            return (
                self.observation_space.sample(),
                float(self.reward_space.sample()),
                terminated,
                truncated,
                {},
            )
        else:
            return (
                copy.deepcopy(self.observation_sample),
                float(copy.deepcopy(self.reward_sample)),
                terminated,
                truncated,
                {},
            )


if __name__ == "__main__":
    
    config = {
            "observation_space": Box(
                low=0,
                high=255,
                shape=(72, 128, 3),
                dtype=np.uint8),
            "action_space": Discrete(5),
            "p_terminated": 1e-4,
            "max_episode_len":495
            }
    
    env = MyRandomEnv(config=config)
    state, _ = env.reset()
    
    counter = 0
    while True:
    # for _ in range(50):
        action = env.action_space.sample()  # agent policy that uses the observation and info
        observation, reward, terminated, truncated, info = env.step(action)
        counter += 1

        if terminated or truncated:
            break
            # observation, info = env.reset()

    env.close()